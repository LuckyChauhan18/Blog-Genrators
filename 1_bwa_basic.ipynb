{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb21e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab69ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "820c0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13b5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fccdd747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "972308e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cacdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"Create a blog plan with 5-7 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2d31d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fanout(state: State):\n",
    "    return [Send(\"worker\", {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "            for task in state[\"plan\"].tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12c0bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    # payload contains what we sent\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Markdown section.\"),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {blog_title}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Brief: {task.brief}\\n\\n\"\n",
    "                    \"Return only the section content in Markdown.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cb4122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    output_dir = Path(\"outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # ðŸ”¥ sanitize title + force .md extension\n",
    "    clean_title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
    "    slug = clean_title.lower().replace(\" \", \"_\")\n",
    "\n",
    "    filename = f\"{slug}.md\"   # ðŸ‘ˆ THIS WAS MISSING\n",
    "    output_path = output_dir / filename\n",
    "\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved file:\", output_path.resolve())\n",
    "    print(\"Size:\", output_path.stat().st_size)\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bde21a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c2c98233b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b5f50b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c2c98233b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14105a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001C2C97BF860>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = g.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaa7a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: C:\\Users\\Lucky\\OneDrive\\Desktop\\AI\\Blog Genrator\\outputs\\understanding_self-attention_the_key_to_modern_nlp.md\n",
      "Size: 17970\n"
     ]
    }
   ],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21ae2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention: The Key to Modern NLP', tasks=[Task(id=1, title='Introduction to Self-Attention', brief='Define self-attention and its significance in natural language processing (NLP). Discuss its role in transforming how models understand context.'), Task(id=2, title='How Self-Attention Works', brief='Explain the mechanics of self-attention, including the concepts of queries, keys, and values. Use diagrams or examples to illustrate the process.'), Task(id=3, title='Self-Attention vs. Traditional Attention Mechanisms', brief='Compare and contrast self-attention with traditional attention mechanisms. Highlight the advantages of self-attention in handling long-range dependencies.'), Task(id=4, title='Applications of Self-Attention in NLP', brief='Explore various applications of self-attention in NLP tasks such as translation, summarization, and sentiment analysis. Provide examples of models that utilize self-attention.'), Task(id=5, title='The Transformer Architecture and Self-Attention', brief=\"Discuss the role of self-attention in the Transformer architecture. Explain how it contributes to the model's performance and efficiency.\"), Task(id=6, title='Challenges and Limitations of Self-Attention', brief='Address some of the challenges and limitations associated with self-attention, including computational complexity and scalability.'), Task(id=7, title='Future of Self-Attention in AI', brief='Speculate on the future developments of self-attention mechanisms in AI and NLP. Discuss potential improvements and emerging trends.')]),\n",
       " 'sections': ['## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence relative to each other, enabling it to capture contextual relationships more effectively. Unlike traditional models that process words sequentially, self-attention evaluates all words simultaneously, allowing for a more holistic understanding of context.\\n\\nIn natural language processing (NLP), self-attention plays a crucial role in transforming how models interpret language. By assigning varying levels of attention to different words based on their relevance to one another, self-attention helps models grasp nuances in meaning, such as ambiguity and polysemy. This capability is particularly significant in tasks like translation, summarization, and sentiment analysis, where understanding the context is essential for generating accurate and coherent outputs.\\n\\nThe introduction of self-attention has led to the development of powerful architectures, such as the Transformer, which rely heavily on this mechanism. As a result, self-attention has become a cornerstone of modern NLP, enabling models to achieve state-of-the-art performance across a wide range of applications.',\n",
       "  '## How Self-Attention Works\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence when encoding a particular word. This process is crucial for understanding context and relationships in natural language processing (NLP). Let\\'s break down the mechanics of self-attention using the concepts of queries, keys, and values.\\n\\n### The Components\\n\\n1. **Queries (Q)**: These are vectors that represent the word for which we want to find relevant information from other words in the sequence.\\n2. **Keys (K)**: These vectors represent the words in the sequence that can provide context or information to the query.\\n3. **Values (V)**: These vectors contain the actual information that will be aggregated based on the attention scores derived from the queries and keys.\\n\\n### The Process\\n\\nThe self-attention mechanism operates through the following steps:\\n\\n1. **Input Representation**: Each word in the input sequence is first transformed into a vector representation (embedding).\\n\\n2. **Creating Queries, Keys, and Values**: For each word, we create a query, key, and value vector by multiplying the input embeddings with learned weight matrices:\\n   - \\\\( Q = XW_Q \\\\)\\n   - \\\\( K = XW_K \\\\)\\n   - \\\\( V = XW_V \\\\)\\n\\n   Here, \\\\( X \\\\) is the input matrix, and \\\\( W_Q, W_K, W_V \\\\) are the weight matrices for queries, keys, and values, respectively.\\n\\n3. **Calculating Attention Scores**: The attention scores are computed by taking the dot product of the query vector with all key vectors. This results in a score that indicates how much focus the model should place on each word when processing the current word:\\n   \\\\[\\n   \\\\text{Attention Score} = Q \\\\cdot K^T\\n   \\\\]\\n\\n4. **Scaling the Scores**: To prevent large values from skewing the softmax function, the scores are scaled by the square root of the dimension of the key vectors:\\n   \\\\[\\n   \\\\text{Scaled Score} = \\\\frac{Q \\\\cdot K^T}{\\\\sqrt{d_k}}\\n   \\\\]\\n\\n5. **Applying Softmax**: The scaled scores are passed through a softmax function to convert them into probabilities, which sum to 1:\\n   \\\\[\\n   \\\\text{Attention Weights} = \\\\text{softmax}(\\\\text{Scaled Score})\\n   \\\\]\\n\\n6. **Weighted Sum of Values**: Finally, the attention weights are used to compute a weighted sum of the value vectors, producing the output for the current word:\\n   \\\\[\\n   \\\\text{Output} = \\\\text{Attention Weights} \\\\cdot V\\n   \\\\]\\n\\n### Example\\n\\nConsider the sentence: \"The cat sat on the mat.\" If we are focusing on the word \"sat,\" the self-attention mechanism will compute how much attention to give to \"The,\" \"cat,\" \"on,\" and \"the mat\" based on their relevance to \"sat.\" \\n\\n- The query for \"sat\" will interact with the keys of all other words, producing scores that reflect their importance.\\n- After applying softmax, the model might determine that \"cat\" and \"on\" are more relevant than \"The\" or \"the mat,\" leading to a higher weight for their corresponding value vectors.\\n\\n### Visualization\\n\\nHereâ€™s a simplified diagram to illustrate the self-attention process:\\n\\n```\\nInput: [The, cat, sat, on, the, mat]\\n          |    |    |    |    |    |\\n         Q    K    V    K    V    K\\n          \\\\    |    |    |    |    /\\n           \\\\   |    |    |    |   /\\n            \\\\  |    |    |    |  /\\n             \\\\ |    |    |    | /\\n              \\\\|    |    |    |/\\n              Attention Weights\\n                     |\\n                     V\\n                  Output\\n```\\n\\nIn summary, self-attention allows models to dynamically focus on different parts of the input sequence, enabling a deeper understanding of context and relationships, which is essential for tasks like translation, summarization, and more in modern NLP applications.',\n",
       "  '## Self-Attention vs. Traditional Attention Mechanisms\\n\\nIn the realm of Natural Language Processing (NLP), attention mechanisms have revolutionized how models understand and generate language. Traditional attention mechanisms, such as those used in sequence-to-sequence models, focus on aligning input and output sequences. They compute attention scores based on the relationship between the current output token and all input tokens, allowing the model to selectively focus on relevant parts of the input when generating each output.\\n\\nHowever, self-attention, a concept popularized by the Transformer architecture, takes this idea a step further. Instead of aligning input and output sequences, self-attention computes attention scores within a single sequence. Each token in the input can attend to every other token, enabling the model to capture relationships and dependencies regardless of their distance in the sequence.\\n\\n### Advantages of Self-Attention\\n\\n1. **Handling Long-Range Dependencies**: One of the most significant advantages of self-attention is its ability to manage long-range dependencies effectively. Traditional attention mechanisms can struggle with sequences where relevant information is far apart, as they often rely on fixed-length context windows. In contrast, self-attention allows every token to directly access information from any other token, making it easier to capture relationships that span long distances.\\n\\n2. **Parallelization**: Self-attention mechanisms can be computed in parallel, as they do not rely on sequential processing. This parallelization leads to faster training times and improved efficiency, especially when dealing with large datasets.\\n\\n3. **Dynamic Contextualization**: Self-attention dynamically adjusts the context for each token based on the entire sequence. This means that the representation of a word can change depending on its surrounding words, allowing for a more nuanced understanding of language.\\n\\n4. **Scalability**: Self-attention scales well with longer sequences, as it can handle varying lengths without the need for additional architectural modifications. This flexibility is particularly beneficial in tasks involving diverse input lengths, such as document summarization or translation.\\n\\nIn summary, while traditional attention mechanisms have laid the groundwork for understanding relationships in sequences, self-attention offers a more powerful and flexible approach. Its ability to handle long-range dependencies, coupled with advantages in parallelization and dynamic contextualization, makes it a cornerstone of modern NLP architectures.',\n",
       "  \"## Applications of Self-Attention in NLP\\n\\nSelf-attention has revolutionized the field of Natural Language Processing (NLP) by enabling models to weigh the importance of different words in a sentence relative to each other. This mechanism has found numerous applications across various NLP tasks, enhancing the performance and efficiency of models. Here are some key applications of self-attention:\\n\\n### 1. Machine Translation\\nSelf-attention plays a crucial role in machine translation, allowing models to focus on relevant words in the source language while generating the target language. For instance, the Transformer model, introduced by Vaswani et al. in 2017, utilizes self-attention to capture long-range dependencies in sentences, significantly improving translation quality. Google's BERT and OpenAI's GPT also leverage self-attention mechanisms to enhance their translation capabilities.\\n\\n### 2. Text Summarization\\nIn text summarization, self-attention helps models identify the most important sentences or phrases in a document. By evaluating the relationships between words, models can generate concise summaries that retain the core message of the original text. The BART (Bidirectional and Auto-Regressive Transformers) model, for example, employs self-attention to effectively summarize articles by understanding the context and significance of each sentence.\\n\\n### 3. Sentiment Analysis\\nSelf-attention is also instrumental in sentiment analysis, where understanding the context and nuances of language is essential. Models like RoBERTa and DistilBERT utilize self-attention to analyze the sentiment of a given text by focusing on words that carry emotional weight. This allows them to discern subtle differences in sentiment, leading to more accurate predictions.\\n\\n### 4. Question Answering\\nIn question answering tasks, self-attention enables models to relate questions to relevant parts of a passage. The Transformer architecture allows for dynamic attention, where the model can focus on different parts of the text based on the question posed. Models like T5 (Text-to-Text Transfer Transformer) and ALBERT (A Lite BERT) utilize self-attention to improve their ability to extract answers from context-rich documents.\\n\\n### Conclusion\\nThe applications of self-attention in NLP are vast and varied, contributing to significant advancements in how machines understand and generate human language. As research continues to evolve, we can expect even more innovative uses of self-attention in tackling complex language tasks, further bridging the gap between human and machine communication.\",\n",
       "  '## The Transformer Architecture and Self-Attention\\n\\nThe Transformer architecture, introduced in the groundbreaking paper \"Attention is All You Need\" by Vaswani et al. in 2017, revolutionized the field of Natural Language Processing (NLP). At the heart of this architecture lies the self-attention mechanism, which plays a crucial role in enhancing both the performance and efficiency of the model.\\n\\nSelf-attention allows the Transformer to weigh the importance of different words in a sentence relative to each other, regardless of their position. This is achieved by computing a set of attention scores that determine how much focus each word should receive when processing a particular word. For instance, in the sentence \"The cat sat on the mat,\" self-attention enables the model to recognize that \"cat\" and \"sat\" are more closely related than \"cat\" and \"mat,\" even though they are separated by other words.\\n\\nOne of the key advantages of self-attention is its ability to capture long-range dependencies in text. Traditional models, such as recurrent neural networks (RNNs), often struggle with this due to their sequential nature. In contrast, self-attention processes all words simultaneously, allowing the model to consider the entire context of a sentence at once. This parallelization not only speeds up training but also improves the model\\'s understanding of complex relationships within the data.\\n\\nMoreover, self-attention contributes to the efficiency of the Transformer by reducing the need for extensive feature engineering. Instead of relying on predefined linguistic features, the model learns to identify relevant patterns directly from the data. This adaptability makes Transformers particularly effective for a wide range of NLP tasks, from translation to sentiment analysis.\\n\\nIn summary, self-attention is a fundamental component of the Transformer architecture that enhances its ability to understand and generate human language. By enabling the model to focus on relevant words and their relationships, self-attention significantly boosts both performance and efficiency, paving the way for advancements in NLP applications.',\n",
       "  '## Challenges and Limitations of Self-Attention\\n\\nWhile self-attention has revolutionized natural language processing (NLP) and enabled the development of powerful models like Transformers, it is not without its challenges and limitations. Understanding these issues is crucial for researchers and practitioners looking to optimize their applications of self-attention mechanisms.\\n\\n### Computational Complexity\\n\\nOne of the primary challenges of self-attention is its computational complexity. The self-attention mechanism computes attention scores for every pair of tokens in the input sequence, resulting in a time complexity of \\\\(O(n^2)\\\\), where \\\\(n\\\\) is the length of the input sequence. This quadratic scaling can become a significant bottleneck, especially for long sequences, as it requires substantial computational resources and memory. As a result, processing lengthy documents or real-time applications can be hindered by the inefficiency of self-attention.\\n\\n### Scalability\\n\\nScalability is another critical limitation of self-attention. As the size of the input data increases, the memory requirements for storing attention scores and intermediate representations also grow. This can lead to out-of-memory errors or necessitate the use of specialized hardware, which may not be accessible to all users. Furthermore, the need for large datasets to train self-attention models effectively can limit their applicability in domains where data is scarce.\\n\\n### Contextual Limitations\\n\\nSelf-attention mechanisms also face challenges in capturing long-range dependencies effectively. While they are designed to consider all tokens in the input sequence, the attention scores can sometimes lead to a dilution of important contextual information, especially in very long sequences. This can result in models that struggle to maintain coherence and relevance in their outputs.\\n\\n### Mitigation Strategies\\n\\nTo address these challenges, researchers are exploring various strategies, such as sparse attention mechanisms, which reduce the number of token pairs considered, and hierarchical attention models that break down sequences into manageable chunks. Additionally, techniques like memory-augmented networks and recurrent attention can help improve the efficiency and effectiveness of self-attention in specific contexts.\\n\\nIn conclusion, while self-attention has transformed the landscape of NLP, it is essential to remain aware of its challenges and limitations. Ongoing research and innovation will be crucial in overcoming these hurdles and unlocking the full potential of self-attention in various applications.',\n",
       "  '## Future of Self-Attention in AI\\n\\nAs we look ahead, the future of self-attention mechanisms in artificial intelligence and natural language processing (NLP) appears promising and ripe with potential. Several emerging trends and anticipated improvements are likely to shape the evolution of self-attention models.\\n\\n### Enhanced Efficiency\\n\\nOne of the primary challenges with current self-attention models, particularly in large-scale applications, is their computational intensity. Future developments may focus on optimizing these mechanisms to reduce resource consumption. Techniques such as sparse attention, where only a subset of tokens is attended to, or the use of low-rank approximations, could significantly enhance efficiency without sacrificing performance.\\n\\n### Multimodal Integration\\n\\nAs AI systems increasingly require the ability to process and understand multiple types of dataâ€”such as text, images, and audioâ€”self-attention mechanisms are expected to evolve to handle multimodal inputs more effectively. This could lead to the development of more sophisticated models that leverage self-attention across different modalities, enabling richer and more nuanced understanding and generation of content.\\n\\n### Improved Interpretability\\n\\nWhile self-attention has been praised for its ability to highlight relationships between words in a sentence, there is still a need for greater interpretability in AI models. Future research may focus on developing methods to better visualize and understand how self-attention weights are assigned, providing insights into the decision-making processes of these models. This could enhance trust and transparency in AI applications.\\n\\n### Adaptive Attention Mechanisms\\n\\nThe future may also see the rise of adaptive attention mechanisms that can dynamically adjust their focus based on the context or the specific task at hand. This adaptability could lead to more contextually aware models that perform better across a wider range of applications, from conversational agents to complex reasoning tasks.\\n\\n### Integration with Other AI Paradigms\\n\\nFinally, self-attention mechanisms may increasingly be integrated with other AI paradigms, such as reinforcement learning and unsupervised learning. This could lead to the development of hybrid models that leverage the strengths of self-attention while also incorporating feedback from the environment or learning from unlabelled data, resulting in more robust and versatile AI systems.\\n\\nIn conclusion, the future of self-attention in AI and NLP is poised for significant advancements. As researchers continue to explore new techniques and applications, we can expect self-attention mechanisms to play a central role in the next generation of intelligent systems, driving innovation and enhancing our ability to understand and interact with language and information.'],\n",
       " 'final': '# Understanding Self-Attention: The Key to Modern NLP\\n\\n## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence relative to each other, enabling it to capture contextual relationships more effectively. Unlike traditional models that process words sequentially, self-attention evaluates all words simultaneously, allowing for a more holistic understanding of context.\\n\\nIn natural language processing (NLP), self-attention plays a crucial role in transforming how models interpret language. By assigning varying levels of attention to different words based on their relevance to one another, self-attention helps models grasp nuances in meaning, such as ambiguity and polysemy. This capability is particularly significant in tasks like translation, summarization, and sentiment analysis, where understanding the context is essential for generating accurate and coherent outputs.\\n\\nThe introduction of self-attention has led to the development of powerful architectures, such as the Transformer, which rely heavily on this mechanism. As a result, self-attention has become a cornerstone of modern NLP, enabling models to achieve state-of-the-art performance across a wide range of applications.\\n\\n## How Self-Attention Works\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sentence when encoding a particular word. This process is crucial for understanding context and relationships in natural language processing (NLP). Let\\'s break down the mechanics of self-attention using the concepts of queries, keys, and values.\\n\\n### The Components\\n\\n1. **Queries (Q)**: These are vectors that represent the word for which we want to find relevant information from other words in the sequence.\\n2. **Keys (K)**: These vectors represent the words in the sequence that can provide context or information to the query.\\n3. **Values (V)**: These vectors contain the actual information that will be aggregated based on the attention scores derived from the queries and keys.\\n\\n### The Process\\n\\nThe self-attention mechanism operates through the following steps:\\n\\n1. **Input Representation**: Each word in the input sequence is first transformed into a vector representation (embedding).\\n\\n2. **Creating Queries, Keys, and Values**: For each word, we create a query, key, and value vector by multiplying the input embeddings with learned weight matrices:\\n   - \\\\( Q = XW_Q \\\\)\\n   - \\\\( K = XW_K \\\\)\\n   - \\\\( V = XW_V \\\\)\\n\\n   Here, \\\\( X \\\\) is the input matrix, and \\\\( W_Q, W_K, W_V \\\\) are the weight matrices for queries, keys, and values, respectively.\\n\\n3. **Calculating Attention Scores**: The attention scores are computed by taking the dot product of the query vector with all key vectors. This results in a score that indicates how much focus the model should place on each word when processing the current word:\\n   \\\\[\\n   \\\\text{Attention Score} = Q \\\\cdot K^T\\n   \\\\]\\n\\n4. **Scaling the Scores**: To prevent large values from skewing the softmax function, the scores are scaled by the square root of the dimension of the key vectors:\\n   \\\\[\\n   \\\\text{Scaled Score} = \\\\frac{Q \\\\cdot K^T}{\\\\sqrt{d_k}}\\n   \\\\]\\n\\n5. **Applying Softmax**: The scaled scores are passed through a softmax function to convert them into probabilities, which sum to 1:\\n   \\\\[\\n   \\\\text{Attention Weights} = \\\\text{softmax}(\\\\text{Scaled Score})\\n   \\\\]\\n\\n6. **Weighted Sum of Values**: Finally, the attention weights are used to compute a weighted sum of the value vectors, producing the output for the current word:\\n   \\\\[\\n   \\\\text{Output} = \\\\text{Attention Weights} \\\\cdot V\\n   \\\\]\\n\\n### Example\\n\\nConsider the sentence: \"The cat sat on the mat.\" If we are focusing on the word \"sat,\" the self-attention mechanism will compute how much attention to give to \"The,\" \"cat,\" \"on,\" and \"the mat\" based on their relevance to \"sat.\" \\n\\n- The query for \"sat\" will interact with the keys of all other words, producing scores that reflect their importance.\\n- After applying softmax, the model might determine that \"cat\" and \"on\" are more relevant than \"The\" or \"the mat,\" leading to a higher weight for their corresponding value vectors.\\n\\n### Visualization\\n\\nHereâ€™s a simplified diagram to illustrate the self-attention process:\\n\\n```\\nInput: [The, cat, sat, on, the, mat]\\n          |    |    |    |    |    |\\n         Q    K    V    K    V    K\\n          \\\\    |    |    |    |    /\\n           \\\\   |    |    |    |   /\\n            \\\\  |    |    |    |  /\\n             \\\\ |    |    |    | /\\n              \\\\|    |    |    |/\\n              Attention Weights\\n                     |\\n                     V\\n                  Output\\n```\\n\\nIn summary, self-attention allows models to dynamically focus on different parts of the input sequence, enabling a deeper understanding of context and relationships, which is essential for tasks like translation, summarization, and more in modern NLP applications.\\n\\n## Self-Attention vs. Traditional Attention Mechanisms\\n\\nIn the realm of Natural Language Processing (NLP), attention mechanisms have revolutionized how models understand and generate language. Traditional attention mechanisms, such as those used in sequence-to-sequence models, focus on aligning input and output sequences. They compute attention scores based on the relationship between the current output token and all input tokens, allowing the model to selectively focus on relevant parts of the input when generating each output.\\n\\nHowever, self-attention, a concept popularized by the Transformer architecture, takes this idea a step further. Instead of aligning input and output sequences, self-attention computes attention scores within a single sequence. Each token in the input can attend to every other token, enabling the model to capture relationships and dependencies regardless of their distance in the sequence.\\n\\n### Advantages of Self-Attention\\n\\n1. **Handling Long-Range Dependencies**: One of the most significant advantages of self-attention is its ability to manage long-range dependencies effectively. Traditional attention mechanisms can struggle with sequences where relevant information is far apart, as they often rely on fixed-length context windows. In contrast, self-attention allows every token to directly access information from any other token, making it easier to capture relationships that span long distances.\\n\\n2. **Parallelization**: Self-attention mechanisms can be computed in parallel, as they do not rely on sequential processing. This parallelization leads to faster training times and improved efficiency, especially when dealing with large datasets.\\n\\n3. **Dynamic Contextualization**: Self-attention dynamically adjusts the context for each token based on the entire sequence. This means that the representation of a word can change depending on its surrounding words, allowing for a more nuanced understanding of language.\\n\\n4. **Scalability**: Self-attention scales well with longer sequences, as it can handle varying lengths without the need for additional architectural modifications. This flexibility is particularly beneficial in tasks involving diverse input lengths, such as document summarization or translation.\\n\\nIn summary, while traditional attention mechanisms have laid the groundwork for understanding relationships in sequences, self-attention offers a more powerful and flexible approach. Its ability to handle long-range dependencies, coupled with advantages in parallelization and dynamic contextualization, makes it a cornerstone of modern NLP architectures.\\n\\n## Applications of Self-Attention in NLP\\n\\nSelf-attention has revolutionized the field of Natural Language Processing (NLP) by enabling models to weigh the importance of different words in a sentence relative to each other. This mechanism has found numerous applications across various NLP tasks, enhancing the performance and efficiency of models. Here are some key applications of self-attention:\\n\\n### 1. Machine Translation\\nSelf-attention plays a crucial role in machine translation, allowing models to focus on relevant words in the source language while generating the target language. For instance, the Transformer model, introduced by Vaswani et al. in 2017, utilizes self-attention to capture long-range dependencies in sentences, significantly improving translation quality. Google\\'s BERT and OpenAI\\'s GPT also leverage self-attention mechanisms to enhance their translation capabilities.\\n\\n### 2. Text Summarization\\nIn text summarization, self-attention helps models identify the most important sentences or phrases in a document. By evaluating the relationships between words, models can generate concise summaries that retain the core message of the original text. The BART (Bidirectional and Auto-Regressive Transformers) model, for example, employs self-attention to effectively summarize articles by understanding the context and significance of each sentence.\\n\\n### 3. Sentiment Analysis\\nSelf-attention is also instrumental in sentiment analysis, where understanding the context and nuances of language is essential. Models like RoBERTa and DistilBERT utilize self-attention to analyze the sentiment of a given text by focusing on words that carry emotional weight. This allows them to discern subtle differences in sentiment, leading to more accurate predictions.\\n\\n### 4. Question Answering\\nIn question answering tasks, self-attention enables models to relate questions to relevant parts of a passage. The Transformer architecture allows for dynamic attention, where the model can focus on different parts of the text based on the question posed. Models like T5 (Text-to-Text Transfer Transformer) and ALBERT (A Lite BERT) utilize self-attention to improve their ability to extract answers from context-rich documents.\\n\\n### Conclusion\\nThe applications of self-attention in NLP are vast and varied, contributing to significant advancements in how machines understand and generate human language. As research continues to evolve, we can expect even more innovative uses of self-attention in tackling complex language tasks, further bridging the gap between human and machine communication.\\n\\n## The Transformer Architecture and Self-Attention\\n\\nThe Transformer architecture, introduced in the groundbreaking paper \"Attention is All You Need\" by Vaswani et al. in 2017, revolutionized the field of Natural Language Processing (NLP). At the heart of this architecture lies the self-attention mechanism, which plays a crucial role in enhancing both the performance and efficiency of the model.\\n\\nSelf-attention allows the Transformer to weigh the importance of different words in a sentence relative to each other, regardless of their position. This is achieved by computing a set of attention scores that determine how much focus each word should receive when processing a particular word. For instance, in the sentence \"The cat sat on the mat,\" self-attention enables the model to recognize that \"cat\" and \"sat\" are more closely related than \"cat\" and \"mat,\" even though they are separated by other words.\\n\\nOne of the key advantages of self-attention is its ability to capture long-range dependencies in text. Traditional models, such as recurrent neural networks (RNNs), often struggle with this due to their sequential nature. In contrast, self-attention processes all words simultaneously, allowing the model to consider the entire context of a sentence at once. This parallelization not only speeds up training but also improves the model\\'s understanding of complex relationships within the data.\\n\\nMoreover, self-attention contributes to the efficiency of the Transformer by reducing the need for extensive feature engineering. Instead of relying on predefined linguistic features, the model learns to identify relevant patterns directly from the data. This adaptability makes Transformers particularly effective for a wide range of NLP tasks, from translation to sentiment analysis.\\n\\nIn summary, self-attention is a fundamental component of the Transformer architecture that enhances its ability to understand and generate human language. By enabling the model to focus on relevant words and their relationships, self-attention significantly boosts both performance and efficiency, paving the way for advancements in NLP applications.\\n\\n## Challenges and Limitations of Self-Attention\\n\\nWhile self-attention has revolutionized natural language processing (NLP) and enabled the development of powerful models like Transformers, it is not without its challenges and limitations. Understanding these issues is crucial for researchers and practitioners looking to optimize their applications of self-attention mechanisms.\\n\\n### Computational Complexity\\n\\nOne of the primary challenges of self-attention is its computational complexity. The self-attention mechanism computes attention scores for every pair of tokens in the input sequence, resulting in a time complexity of \\\\(O(n^2)\\\\), where \\\\(n\\\\) is the length of the input sequence. This quadratic scaling can become a significant bottleneck, especially for long sequences, as it requires substantial computational resources and memory. As a result, processing lengthy documents or real-time applications can be hindered by the inefficiency of self-attention.\\n\\n### Scalability\\n\\nScalability is another critical limitation of self-attention. As the size of the input data increases, the memory requirements for storing attention scores and intermediate representations also grow. This can lead to out-of-memory errors or necessitate the use of specialized hardware, which may not be accessible to all users. Furthermore, the need for large datasets to train self-attention models effectively can limit their applicability in domains where data is scarce.\\n\\n### Contextual Limitations\\n\\nSelf-attention mechanisms also face challenges in capturing long-range dependencies effectively. While they are designed to consider all tokens in the input sequence, the attention scores can sometimes lead to a dilution of important contextual information, especially in very long sequences. This can result in models that struggle to maintain coherence and relevance in their outputs.\\n\\n### Mitigation Strategies\\n\\nTo address these challenges, researchers are exploring various strategies, such as sparse attention mechanisms, which reduce the number of token pairs considered, and hierarchical attention models that break down sequences into manageable chunks. Additionally, techniques like memory-augmented networks and recurrent attention can help improve the efficiency and effectiveness of self-attention in specific contexts.\\n\\nIn conclusion, while self-attention has transformed the landscape of NLP, it is essential to remain aware of its challenges and limitations. Ongoing research and innovation will be crucial in overcoming these hurdles and unlocking the full potential of self-attention in various applications.\\n\\n## Future of Self-Attention in AI\\n\\nAs we look ahead, the future of self-attention mechanisms in artificial intelligence and natural language processing (NLP) appears promising and ripe with potential. Several emerging trends and anticipated improvements are likely to shape the evolution of self-attention models.\\n\\n### Enhanced Efficiency\\n\\nOne of the primary challenges with current self-attention models, particularly in large-scale applications, is their computational intensity. Future developments may focus on optimizing these mechanisms to reduce resource consumption. Techniques such as sparse attention, where only a subset of tokens is attended to, or the use of low-rank approximations, could significantly enhance efficiency without sacrificing performance.\\n\\n### Multimodal Integration\\n\\nAs AI systems increasingly require the ability to process and understand multiple types of dataâ€”such as text, images, and audioâ€”self-attention mechanisms are expected to evolve to handle multimodal inputs more effectively. This could lead to the development of more sophisticated models that leverage self-attention across different modalities, enabling richer and more nuanced understanding and generation of content.\\n\\n### Improved Interpretability\\n\\nWhile self-attention has been praised for its ability to highlight relationships between words in a sentence, there is still a need for greater interpretability in AI models. Future research may focus on developing methods to better visualize and understand how self-attention weights are assigned, providing insights into the decision-making processes of these models. This could enhance trust and transparency in AI applications.\\n\\n### Adaptive Attention Mechanisms\\n\\nThe future may also see the rise of adaptive attention mechanisms that can dynamically adjust their focus based on the context or the specific task at hand. This adaptability could lead to more contextually aware models that perform better across a wider range of applications, from conversational agents to complex reasoning tasks.\\n\\n### Integration with Other AI Paradigms\\n\\nFinally, self-attention mechanisms may increasingly be integrated with other AI paradigms, such as reinforcement learning and unsupervised learning. This could lead to the development of hybrid models that leverage the strengths of self-attention while also incorporating feedback from the environment or learning from unlabelled data, resulting in more robust and versatile AI systems.\\n\\nIn conclusion, the future of self-attention in AI and NLP is poised for significant advancements. As researchers continue to explore new techniques and applications, we can expect self-attention mechanisms to play a central role in the next generation of intelligent systems, driving innovation and enhancing our ability to understand and interact with language and information.\\n'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5957d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blogGenration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
