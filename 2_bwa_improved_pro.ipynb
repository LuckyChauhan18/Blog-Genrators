{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831aaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16869bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=5,\n",
    "        description=\"3â€“5 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120â€“450).\",\n",
    "    )\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2995e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description=\"Who this blog is for.\")\n",
    "    tone: str = Field(..., description=\"Writing tone (e.g., practical, crisp).\")\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854662fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]  # reducer concatenates worker outputs\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b95b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14594d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 5â€“7 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3â€“5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120â€“450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem â†’ intuition â†’ approach â†’ implementation â†’ \"\n",
    "                    \"trade-offs â†’ testing/observability â†’ conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7271e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]},\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905354c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "    content=(\n",
    "        \"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
    "        \"Hard constraints:\\n\"\n",
    "        \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
    "        \"- Stay close to the Target words (Â±15%).\\n\"\n",
    "        \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
    "        \"Technical quality bar:\\n\"\n",
    "        \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "        \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "        \"- When relevant, include at least one of:\\n\"\n",
    "        \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "        \"  * a tiny example input/output\\n\"\n",
    "        \"  * a checklist of steps\\n\"\n",
    "        \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "        \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "        \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "        \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "        \"Markdown style:\\n\"\n",
    "        \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "        \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "        \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "        \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "    )\n",
    ")\n",
    ",\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Section type: {task.section_type}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19c3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    output_dir = Path(\"outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # ðŸ”¥ sanitize title + force .md extension\n",
    "    clean_title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
    "    slug = clean_title.lower().replace(\" \", \"_\")\n",
    "\n",
    "    filename = f\"{slug}.md\"   # ðŸ‘ˆ THIS WAS MISSING\n",
    "    output_path = output_dir / filename\n",
    "\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved file:\", output_path.resolve())\n",
    "    print(\"Size:\", output_path.stat().st_size)\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3023a91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x175a15168a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c94e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000175A1926FC0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = g.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e31a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: C:\\Users\\Lucky\\OneDrive\\Desktop\\AI\\Blog Genrator\\outputs\\understanding_self-attention_the_key_to_modern_nlp.md\n",
      "Size: 19285\n"
     ]
    }
   ],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fee5a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention: The Key to Modern NLP', audience='Developers and Data Scientists', tone='practical, crisp', tasks=[Task(id=1, title='Introduction to Self-Attention', goal='Understand the concept and significance of self-attention in natural language processing (NLP).', bullets=['Define self-attention and its role in transforming input sequences into context-aware representations.', 'Discuss the limitations of traditional sequence models like RNNs and LSTMs in capturing long-range dependencies.', 'Introduce the concept of attention mechanisms and how self-attention improves upon them.'], target_words=300, section_type='intro'), Task(id=2, title='The Mechanics of Self-Attention', goal='Learn how self-attention computes attention scores and generates context-aware embeddings.', bullets=['Explain the mathematical formulation of self-attention, including queries, keys, and values.', 'Provide a minimal working example (MWE) in Python using NumPy to illustrate the self-attention calculation.', 'Discuss the role of the softmax function in normalizing attention scores.'], target_words=400, section_type='core'), Task(id=3, title='Implementing Self-Attention in PyTorch', goal='Implement a self-attention layer using PyTorch and understand its components.', bullets=['Show a code sketch for a self-attention layer in PyTorch, including initialization and forward methods.', 'Explain how to integrate the self-attention layer into a simple transformer model architecture.', 'Discuss the importance of multi-head attention and how to implement it alongside self-attention.'], target_words=450, section_type='core'), Task(id=4, title='Common Mistakes in Self-Attention Implementation', goal='Identify and avoid common pitfalls when implementing self-attention mechanisms.', bullets=['Highlight the importance of correctly shaping input tensors to avoid dimension mismatches.', 'Discuss the impact of not normalizing attention scores and how it can lead to poor model performance.', 'Provide debugging tips for tracking down issues in attention score calculations, such as using print statements or logging.'], target_words=300, section_type='common_mistakes'), Task(id=5, title='Performance Considerations for Self-Attention', goal='Understand the computational complexity and performance trade-offs of self-attention mechanisms.', bullets=['Analyze the time and space complexity of self-attention, particularly in relation to sequence length.', 'Discuss strategies for optimizing self-attention, such as using sparse attention or kernelized methods.', 'Provide benchmarks comparing self-attention with traditional RNNs and LSTMs on various tasks.'], target_words=400, section_type='core'), Task(id=6, title='Testing and Observability in Self-Attention Models', goal='Learn how to effectively test and monitor self-attention models in production.', bullets=['Outline key metrics to track for self-attention models, such as attention distribution and loss curves.', 'Discuss the importance of visualizing attention weights to interpret model behavior and performance.', 'Provide a checklist for setting up logging and monitoring for self-attention models in a production environment.'], target_words=350, section_type='core'), Task(id=7, title='Conclusion and Next Steps', goal='Summarize key takeaways and suggest further learning resources on self-attention.', bullets=['Recap the importance of self-attention in modern NLP and its advantages over traditional methods.', 'Suggest advanced topics for further exploration, such as transformers and BERT.', 'Provide a checklist for implementing self-attention in new projects, including best practices and resources.'], target_words=250, section_type='conclusion')]),\n",
       " 'sections': ['## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding them into context-aware representations. Unlike traditional models, self-attention computes a representation of a word by considering all other words in the input sequence, enabling it to capture nuanced relationships and dependencies. This is particularly significant in natural language processing (NLP), where the meaning of a word can depend heavily on its context.\\n\\nTraditional sequence models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), struggle with long-range dependencies due to their sequential nature. They process input tokens one at a time, which can lead to issues like vanishing gradients and difficulty in retaining information from earlier tokens. For example, in the sentence \"The cat that chased the mouse was fast,\" an RNN may have trouble associating \"cat\" with \"fast\" because of the intervening words. This limitation can hinder performance on tasks requiring an understanding of context over longer distances.\\n\\nAttention mechanisms were introduced to address these limitations by allowing models to focus on specific parts of the input sequence when making predictions. However, traditional attention mechanisms still rely on sequential processing, which can be inefficient. Self-attention improves upon this by enabling parallel computation, allowing the model to consider all words simultaneously. This results in a more efficient representation of the input sequence, as each word can directly attend to every other word, regardless of their position.\\n\\nIn practice, self-attention computes three vectors for each word: the Query (Q), Key (K), and Value (V). The attention score is calculated using the dot product of the Query and Key vectors, followed by a softmax operation to normalize the scores. The final output is a weighted sum of the Value vectors, where the weights are determined by the attention scores. This mechanism allows the model to dynamically adjust its focus based on the context, leading to richer and more informative representations.\\n\\nIn summary, self-attention is a powerful tool in NLP that overcomes the limitations of traditional sequence models by enabling context-aware representations through efficient parallel processing.',\n",
       "  '## The Mechanics of Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding a particular word. The core components of self-attention are queries, keys, and values, which are derived from the input embeddings.\\n\\n### Mathematical Formulation\\n\\nGiven an input sequence represented as a matrix \\\\( X \\\\) of shape \\\\( (n, d) \\\\), where \\\\( n \\\\) is the number of tokens and \\\\( d \\\\) is the embedding dimension, we compute the following:\\n\\n1. **Queries (Q)**: \\\\( Q = XW_Q \\\\)\\n2. **Keys (K)**: \\\\( K = XW_K \\\\)\\n3. **Values (V)**: \\\\( V = XW_V \\\\)\\n\\nHere, \\\\( W_Q \\\\), \\\\( W_K \\\\), and \\\\( W_V \\\\) are weight matrices of shape \\\\( (d, d_k) \\\\), where \\\\( d_k \\\\) is the dimension of the keys and queries. The attention scores are computed as:\\n\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\\n\\\\]\\n\\nThe division by \\\\( \\\\sqrt{d_k} \\\\) helps to stabilize gradients during training by scaling the dot products.\\n\\n### Minimal Working Example in Python\\n\\nHereâ€™s a minimal working example using NumPy to illustrate the self-attention calculation:\\n\\n```python\\nimport numpy as np\\n\\n# Input sequence (3 tokens, 4-dimensional embeddings)\\nX = np.array([[1, 0, 1, 0],\\n              [0, 1, 0, 1],\\n              [1, 1, 1, 1]])\\n\\n# Weight matrices (for simplicity, using identity matrices)\\nW_Q = np.eye(4)\\nW_K = np.eye(4)\\nW_V = np.eye(4)\\n\\n# Compute Q, K, V\\nQ = X @ W_Q\\nK = X @ W_K\\nV = X @ W_V\\n\\n# Compute attention scores\\nscores = Q @ K.T / np.sqrt(Q.shape[1])\\nattention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\\n\\n# Compute the output\\noutput = attention_weights @ V\\nprint(\"Attention Output:\\\\n\", output)\\n```\\n\\n### Role of the Softmax Function\\n\\nThe softmax function is crucial in self-attention as it normalizes the attention scores into a probability distribution. This ensures that the weights assigned to the values sum to 1, allowing the model to focus on the most relevant parts of the input sequence. \\n\\n- **Why Use Softmax?**: It transforms raw scores into a range between 0 and 1, making it easier to interpret the importance of each token relative to others.\\n\\n### Trade-offs and Edge Cases\\n\\n- **Performance**: Self-attention has a time complexity of \\\\( O(n^2) \\\\), which can be a bottleneck for long sequences. Techniques like sparse attention or limiting the context window can mitigate this.\\n- **Cost**: The memory requirement grows quadratically with the sequence length, which can lead to out-of-memory errors for large inputs. Consider using batching or truncating sequences.\\n- **Reliability**: Ensure that the input embeddings are well-distributed; otherwise, the attention mechanism may focus on irrelevant tokens. Regularization techniques can help improve robustness.\\n\\nBy understanding these mechanics, developers can effectively implement self-attention in their NLP models, enhancing their ability to capture contextual relationships.',\n",
       "  '## Implementing Self-Attention in PyTorch\\n\\nTo implement a self-attention layer in PyTorch, we need to define a class that includes the initialization and forward methods. Below is a code sketch for a basic self-attention layer.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_size, heads):\\n        super(SelfAttention, self).__init__()\\n        self.embed_size = embed_size\\n        self.heads = heads\\n        self.head_dim = embed_size // heads\\n\\n        assert (\\n            self.head_dim * heads == embed_size\\n        ), \"Embedding size must be divisible by heads\"\\n\\n        self.values = nn.Linear(embed_size, embed_size, bias=False)\\n        self.keys = nn.Linear(embed_size, embed_size, bias=False)\\n        self.queries = nn.Linear(embed_size, embed_size, bias=False)\\n        self.fc_out = nn.Linear(embed_size, embed_size)\\n\\n    def forward(self, x):\\n        N, seq_length, _ = x.shape\\n        values = self.values(x)\\n        keys = self.keys(x)\\n        queries = self.queries(x)\\n\\n        values = values.view(N, seq_length, self.heads, self.head_dim)\\n        keys = keys.view(N, seq_length, self.heads, self.head_dim)\\n        queries = queries.view(N, seq_length, self.heads, self.head_dim)\\n\\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\\n        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\\n\\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\\n            N, seq_length, self.embed_size\\n        )\\n        return self.fc_out(out)\\n```\\n\\n### Integrating Self-Attention into a Transformer Model\\n\\nTo integrate the self-attention layer into a simple transformer model, we can create a class that includes an encoder and decoder. The self-attention layer will be used in the encoder.\\n\\n```python\\nclass Transformer(nn.Module):\\n    def __init__(self, embed_size, heads, num_layers):\\n        super(Transformer, self).__init__()\\n        self.encoder_layers = nn.ModuleList(\\n            [SelfAttention(embed_size, heads) for _ in range(num_layers)]\\n        )\\n\\n    def forward(self, x):\\n        for layer in self.encoder_layers:\\n            x = layer(x)\\n        return x\\n```\\n\\n### Importance of Multi-Head Attention\\n\\nMulti-head attention allows the model to focus on different parts of the input sequence simultaneously, capturing various relationships. This is crucial for understanding context in natural language processing tasks. \\n\\nTo implement multi-head attention alongside self-attention, we can modify the `SelfAttention` class to handle multiple heads. The code provided already incorporates this by splitting the embedding size into multiple heads and processing them in parallel.\\n\\n#### Trade-offs\\n\\n- **Performance**: Multi-head attention can be computationally expensive due to the increased number of parameters and operations.\\n- **Complexity**: The implementation becomes more complex as you need to manage multiple linear transformations and attention scores.\\n- **Reliability**: Proper initialization and normalization are essential to ensure stable training.\\n\\n#### Edge Cases\\n\\n- If the embedding size is not divisible by the number of heads, an assertion error will be raised. Ensure to validate these parameters before instantiation.\\n- If the input sequence length is zero, the model will not function correctly. Always check for valid input shapes.\\n\\nBy following this structure, you can effectively implement a self-attention layer and integrate it into a transformer model, leveraging the power of multi-head attention for improved performance in NLP tasks.',\n",
       "  \"## Common Mistakes in Self-Attention Implementation\\n\\nWhen implementing self-attention mechanisms, developers often encounter pitfalls that can lead to suboptimal performance or runtime errors. Here are some common mistakes to avoid:\\n\\n### 1. Incorrect Shaping of Input Tensors\\n\\nOne of the most critical aspects of self-attention is ensuring that input tensors are correctly shaped. The input to the self-attention layer should typically have the shape `(batch_size, sequence_length, embedding_dim)`. A common mistake is mismatching these dimensions, which can lead to runtime errors or incorrect calculations.\\n\\n**Checklist for Input Tensor Shape:**\\n- Ensure `batch_size` is consistent across all inputs.\\n- Verify that `sequence_length` matches the length of the input sequences.\\n- Confirm that `embedding_dim` aligns with the model's architecture.\\n\\n### 2. Not Normalizing Attention Scores\\n\\nFailing to normalize attention scores can severely impact model performance. The attention scores are computed using the dot product of query and key vectors, and they should be scaled by the square root of the dimension of the key vectors. Without normalization, the scores can become too large, leading to softmax saturation and poor gradient flow.\\n\\n**Example of Normalization:**\\n```python\\nimport torch\\n\\ndef scaled_dot_product_attention(query, key, value):\\n    d_k = query.size(-1)\\n    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\\n    attention_weights = torch.softmax(scores, dim=-1)\\n    return torch.matmul(attention_weights, value)\\n```\\n\\n### 3. Debugging Attention Score Calculations\\n\\nDebugging issues in attention score calculations can be challenging. A practical approach is to use print statements or logging to track the intermediate values. For instance, log the shapes of the query, key, and value tensors, as well as the computed attention scores.\\n\\n**Debugging Tips:**\\n- Use `print(query.shape, key.shape, value.shape)` to verify tensor dimensions.\\n- Log the attention scores before applying softmax to check for extreme values.\\n- Consider using assertions to ensure that the shapes of the tensors are as expected.\\n\\nBy being mindful of these common mistakes, you can enhance the reliability and performance of your self-attention implementations.\",\n",
       "  \"## Performance Considerations for Self-Attention\\n\\nSelf-attention mechanisms have revolutionized natural language processing (NLP), but they come with significant computational costs. Understanding these costs is crucial for optimizing performance in real-world applications.\\n\\n### Time and Space Complexity\\n\\nThe time complexity of self-attention is \\\\(O(n^2 \\\\cdot d)\\\\), where \\\\(n\\\\) is the sequence length and \\\\(d\\\\) is the dimensionality of the input embeddings. This quadratic relationship arises because each token in the sequence attends to every other token, resulting in a full attention matrix of size \\\\(n \\\\times n\\\\). The space complexity is also \\\\(O(n^2)\\\\) due to the storage of this attention matrix.\\n\\nFor example, consider a sequence of 512 tokens with an embedding size of 768. The attention matrix would require approximately 196,608,000 entries, which can be prohibitive for longer sequences.\\n\\n### Optimization Strategies\\n\\nTo mitigate the high computational costs, several strategies can be employed:\\n\\n- **Sparse Attention**: Instead of computing attention for all pairs of tokens, sparse attention mechanisms focus on a subset. Techniques like local attention (only attending to nearby tokens) or global attention (attending to specific tokens) can reduce complexity to \\\\(O(n \\\\cdot d \\\\cdot k)\\\\), where \\\\(k\\\\) is the number of attended tokens.\\n\\n- **Kernelized Methods**: These methods approximate the attention mechanism using kernel functions, reducing the complexity to linear or sub-quadratic. For instance, the Performer model uses positive orthogonal random features to approximate softmax attention, achieving \\\\(O(n \\\\cdot d \\\\cdot \\\\log(n))\\\\) complexity.\\n\\n- **Low-Rank Approximations**: Techniques like Linformer use low-rank projections to reduce the size of the attention matrix, allowing for faster computations while maintaining performance.\\n\\n### Benchmarks Against RNNs and LSTMs\\n\\nWhen comparing self-attention with traditional recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), benchmarks reveal significant differences in performance:\\n\\n- **Translation Tasks**: In machine translation, self-attention models like Transformers outperform LSTMs by achieving higher BLEU scores while training in parallel, significantly reducing training time.\\n\\n- **Text Classification**: Self-attention models can process entire sequences simultaneously, leading to faster inference times compared to RNNs, which must process tokens sequentially.\\n\\n- **Memory Usage**: While RNNs and LSTMs have linear memory requirements, self-attention's quadratic memory usage can become a bottleneck for long sequences. \\n\\n### Trade-offs and Edge Cases\\n\\nWhile self-attention provides superior performance in many tasks, it is essential to consider trade-offs. Sparse and kernelized methods may introduce approximation errors, potentially affecting model accuracy. Additionally, for very long sequences, the memory overhead can lead to out-of-memory errors. \\n\\nTo handle these edge cases, consider implementing gradient checkpointing to save memory during training or using mixed precision training to reduce memory footprint. \\n\\nIn summary, understanding the performance implications of self-attention is vital for effective model design and deployment in NLP applications.\",\n",
       "  \"## Testing and Observability in Self-Attention Models\\n\\nTo ensure the reliability and performance of self-attention models in production, it is crucial to track specific metrics that provide insights into their behavior. Key metrics to monitor include:\\n\\n- **Attention Distribution**: Analyze how attention weights are distributed across different tokens. This helps in understanding which parts of the input the model focuses on.\\n- **Loss Curves**: Track training and validation loss over epochs. A decreasing loss indicates that the model is learning effectively, while a plateau or increase may signal issues such as overfitting.\\n\\nVisualizing attention weights is essential for interpreting model behavior. By plotting attention distributions, you can identify patterns and anomalies. For instance, if a model consistently attends to irrelevant tokens, it may indicate a need for further training or architectural adjustments. Use libraries like Matplotlib or Seaborn to create heatmaps of attention weights:\\n\\n```python\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndef plot_attention_weights(attention_weights):\\n    plt.figure(figsize=(10, 8))\\n    sns.heatmap(attention_weights, cmap='viridis')\\n    plt.title('Attention Weights Heatmap')\\n    plt.xlabel('Input Tokens')\\n    plt.ylabel('Output Tokens')\\n    plt.show()\\n```\\n\\nSetting up logging and monitoring for self-attention models in a production environment is critical for maintaining performance. Hereâ€™s a checklist to guide you:\\n\\n1. **Log Model Predictions**: Capture input data, predictions, and actual outcomes to analyze model performance over time.\\n2. **Monitor Latency**: Measure the time taken for inference requests to ensure they meet performance requirements.\\n3. **Track Resource Utilization**: Monitor CPU, GPU, and memory usage to identify potential bottlenecks.\\n4. **Set Up Alerts**: Configure alerts for significant deviations in loss or latency metrics to respond quickly to issues.\\n5. **Version Control**: Maintain versioning of models and datasets to facilitate rollback in case of performance degradation.\\n\\nBy implementing these practices, you can enhance the observability of self-attention models, allowing for timely interventions and continuous improvement. This proactive approach not only boosts reliability but also aids in understanding model behavior, ultimately leading to better performance in real-world applications.\",\n",
       "  '## Conclusion and Next Steps\\n\\nSelf-attention has revolutionized natural language processing (NLP) by allowing models to weigh the significance of different words in a sentence relative to each other. Unlike traditional methods, which often rely on fixed context windows or sequential processing, self-attention enables models to capture long-range dependencies and contextual relationships more effectively. This leads to improved performance in tasks such as translation, summarization, and sentiment analysis.\\n\\nFor those looking to deepen their understanding of self-attention, consider exploring advanced topics such as:\\n\\n- **Transformers**: The architecture that popularized self-attention, enabling parallel processing of input sequences.\\n- **BERT (Bidirectional Encoder Representations from Transformers)**: A model that leverages self-attention for context-aware embeddings, significantly enhancing various NLP tasks.\\n\\nTo implement self-attention in your projects, follow this checklist:\\n\\n1. **Understand the Mechanism**: Familiarize yourself with the self-attention formula:\\n   ```python\\n   Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V\\n   ```\\n2. **Choose a Framework**: Select a deep learning framework like TensorFlow or PyTorch that supports self-attention layers.\\n3. **Preprocess Data**: Tokenize and encode your text data appropriately, ensuring that you handle padding and masking.\\n4. **Implement the Layer**: Use built-in self-attention layers or create a custom implementation based on your needs.\\n5. **Tune Hyperparameters**: Experiment with the number of attention heads, layer sizes, and dropout rates to optimize performance.\\n6. **Evaluate and Iterate**: Continuously assess model performance on validation datasets and refine your approach.\\n\\nFor further resources, consider the original paper on Transformers, online courses on NLP, and GitHub repositories with self-attention implementations. Understanding these concepts will enhance your ability to leverage self-attention effectively in your projects.'],\n",
       " 'final': '# Understanding Self-Attention: The Key to Modern NLP\\n\\n## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding them into context-aware representations. Unlike traditional models, self-attention computes a representation of a word by considering all other words in the input sequence, enabling it to capture nuanced relationships and dependencies. This is particularly significant in natural language processing (NLP), where the meaning of a word can depend heavily on its context.\\n\\nTraditional sequence models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), struggle with long-range dependencies due to their sequential nature. They process input tokens one at a time, which can lead to issues like vanishing gradients and difficulty in retaining information from earlier tokens. For example, in the sentence \"The cat that chased the mouse was fast,\" an RNN may have trouble associating \"cat\" with \"fast\" because of the intervening words. This limitation can hinder performance on tasks requiring an understanding of context over longer distances.\\n\\nAttention mechanisms were introduced to address these limitations by allowing models to focus on specific parts of the input sequence when making predictions. However, traditional attention mechanisms still rely on sequential processing, which can be inefficient. Self-attention improves upon this by enabling parallel computation, allowing the model to consider all words simultaneously. This results in a more efficient representation of the input sequence, as each word can directly attend to every other word, regardless of their position.\\n\\nIn practice, self-attention computes three vectors for each word: the Query (Q), Key (K), and Value (V). The attention score is calculated using the dot product of the Query and Key vectors, followed by a softmax operation to normalize the scores. The final output is a weighted sum of the Value vectors, where the weights are determined by the attention scores. This mechanism allows the model to dynamically adjust its focus based on the context, leading to richer and more informative representations.\\n\\nIn summary, self-attention is a powerful tool in NLP that overcomes the limitations of traditional sequence models by enabling context-aware representations through efficient parallel processing.\\n\\n## The Mechanics of Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to weigh the importance of different words in a sequence when encoding a particular word. The core components of self-attention are queries, keys, and values, which are derived from the input embeddings.\\n\\n### Mathematical Formulation\\n\\nGiven an input sequence represented as a matrix \\\\( X \\\\) of shape \\\\( (n, d) \\\\), where \\\\( n \\\\) is the number of tokens and \\\\( d \\\\) is the embedding dimension, we compute the following:\\n\\n1. **Queries (Q)**: \\\\( Q = XW_Q \\\\)\\n2. **Keys (K)**: \\\\( K = XW_K \\\\)\\n3. **Values (V)**: \\\\( V = XW_V \\\\)\\n\\nHere, \\\\( W_Q \\\\), \\\\( W_K \\\\), and \\\\( W_V \\\\) are weight matrices of shape \\\\( (d, d_k) \\\\), where \\\\( d_k \\\\) is the dimension of the keys and queries. The attention scores are computed as:\\n\\n\\\\[\\n\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V\\n\\\\]\\n\\nThe division by \\\\( \\\\sqrt{d_k} \\\\) helps to stabilize gradients during training by scaling the dot products.\\n\\n### Minimal Working Example in Python\\n\\nHereâ€™s a minimal working example using NumPy to illustrate the self-attention calculation:\\n\\n```python\\nimport numpy as np\\n\\n# Input sequence (3 tokens, 4-dimensional embeddings)\\nX = np.array([[1, 0, 1, 0],\\n              [0, 1, 0, 1],\\n              [1, 1, 1, 1]])\\n\\n# Weight matrices (for simplicity, using identity matrices)\\nW_Q = np.eye(4)\\nW_K = np.eye(4)\\nW_V = np.eye(4)\\n\\n# Compute Q, K, V\\nQ = X @ W_Q\\nK = X @ W_K\\nV = X @ W_V\\n\\n# Compute attention scores\\nscores = Q @ K.T / np.sqrt(Q.shape[1])\\nattention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\\n\\n# Compute the output\\noutput = attention_weights @ V\\nprint(\"Attention Output:\\\\n\", output)\\n```\\n\\n### Role of the Softmax Function\\n\\nThe softmax function is crucial in self-attention as it normalizes the attention scores into a probability distribution. This ensures that the weights assigned to the values sum to 1, allowing the model to focus on the most relevant parts of the input sequence. \\n\\n- **Why Use Softmax?**: It transforms raw scores into a range between 0 and 1, making it easier to interpret the importance of each token relative to others.\\n\\n### Trade-offs and Edge Cases\\n\\n- **Performance**: Self-attention has a time complexity of \\\\( O(n^2) \\\\), which can be a bottleneck for long sequences. Techniques like sparse attention or limiting the context window can mitigate this.\\n- **Cost**: The memory requirement grows quadratically with the sequence length, which can lead to out-of-memory errors for large inputs. Consider using batching or truncating sequences.\\n- **Reliability**: Ensure that the input embeddings are well-distributed; otherwise, the attention mechanism may focus on irrelevant tokens. Regularization techniques can help improve robustness.\\n\\nBy understanding these mechanics, developers can effectively implement self-attention in their NLP models, enhancing their ability to capture contextual relationships.\\n\\n## Implementing Self-Attention in PyTorch\\n\\nTo implement a self-attention layer in PyTorch, we need to define a class that includes the initialization and forward methods. Below is a code sketch for a basic self-attention layer.\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass SelfAttention(nn.Module):\\n    def __init__(self, embed_size, heads):\\n        super(SelfAttention, self).__init__()\\n        self.embed_size = embed_size\\n        self.heads = heads\\n        self.head_dim = embed_size // heads\\n\\n        assert (\\n            self.head_dim * heads == embed_size\\n        ), \"Embedding size must be divisible by heads\"\\n\\n        self.values = nn.Linear(embed_size, embed_size, bias=False)\\n        self.keys = nn.Linear(embed_size, embed_size, bias=False)\\n        self.queries = nn.Linear(embed_size, embed_size, bias=False)\\n        self.fc_out = nn.Linear(embed_size, embed_size)\\n\\n    def forward(self, x):\\n        N, seq_length, _ = x.shape\\n        values = self.values(x)\\n        keys = self.keys(x)\\n        queries = self.queries(x)\\n\\n        values = values.view(N, seq_length, self.heads, self.head_dim)\\n        keys = keys.view(N, seq_length, self.heads, self.head_dim)\\n        queries = queries.view(N, seq_length, self.heads, self.head_dim)\\n\\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\\n        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\\n\\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\\n            N, seq_length, self.embed_size\\n        )\\n        return self.fc_out(out)\\n```\\n\\n### Integrating Self-Attention into a Transformer Model\\n\\nTo integrate the self-attention layer into a simple transformer model, we can create a class that includes an encoder and decoder. The self-attention layer will be used in the encoder.\\n\\n```python\\nclass Transformer(nn.Module):\\n    def __init__(self, embed_size, heads, num_layers):\\n        super(Transformer, self).__init__()\\n        self.encoder_layers = nn.ModuleList(\\n            [SelfAttention(embed_size, heads) for _ in range(num_layers)]\\n        )\\n\\n    def forward(self, x):\\n        for layer in self.encoder_layers:\\n            x = layer(x)\\n        return x\\n```\\n\\n### Importance of Multi-Head Attention\\n\\nMulti-head attention allows the model to focus on different parts of the input sequence simultaneously, capturing various relationships. This is crucial for understanding context in natural language processing tasks. \\n\\nTo implement multi-head attention alongside self-attention, we can modify the `SelfAttention` class to handle multiple heads. The code provided already incorporates this by splitting the embedding size into multiple heads and processing them in parallel.\\n\\n#### Trade-offs\\n\\n- **Performance**: Multi-head attention can be computationally expensive due to the increased number of parameters and operations.\\n- **Complexity**: The implementation becomes more complex as you need to manage multiple linear transformations and attention scores.\\n- **Reliability**: Proper initialization and normalization are essential to ensure stable training.\\n\\n#### Edge Cases\\n\\n- If the embedding size is not divisible by the number of heads, an assertion error will be raised. Ensure to validate these parameters before instantiation.\\n- If the input sequence length is zero, the model will not function correctly. Always check for valid input shapes.\\n\\nBy following this structure, you can effectively implement a self-attention layer and integrate it into a transformer model, leveraging the power of multi-head attention for improved performance in NLP tasks.\\n\\n## Common Mistakes in Self-Attention Implementation\\n\\nWhen implementing self-attention mechanisms, developers often encounter pitfalls that can lead to suboptimal performance or runtime errors. Here are some common mistakes to avoid:\\n\\n### 1. Incorrect Shaping of Input Tensors\\n\\nOne of the most critical aspects of self-attention is ensuring that input tensors are correctly shaped. The input to the self-attention layer should typically have the shape `(batch_size, sequence_length, embedding_dim)`. A common mistake is mismatching these dimensions, which can lead to runtime errors or incorrect calculations.\\n\\n**Checklist for Input Tensor Shape:**\\n- Ensure `batch_size` is consistent across all inputs.\\n- Verify that `sequence_length` matches the length of the input sequences.\\n- Confirm that `embedding_dim` aligns with the model\\'s architecture.\\n\\n### 2. Not Normalizing Attention Scores\\n\\nFailing to normalize attention scores can severely impact model performance. The attention scores are computed using the dot product of query and key vectors, and they should be scaled by the square root of the dimension of the key vectors. Without normalization, the scores can become too large, leading to softmax saturation and poor gradient flow.\\n\\n**Example of Normalization:**\\n```python\\nimport torch\\n\\ndef scaled_dot_product_attention(query, key, value):\\n    d_k = query.size(-1)\\n    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\\n    attention_weights = torch.softmax(scores, dim=-1)\\n    return torch.matmul(attention_weights, value)\\n```\\n\\n### 3. Debugging Attention Score Calculations\\n\\nDebugging issues in attention score calculations can be challenging. A practical approach is to use print statements or logging to track the intermediate values. For instance, log the shapes of the query, key, and value tensors, as well as the computed attention scores.\\n\\n**Debugging Tips:**\\n- Use `print(query.shape, key.shape, value.shape)` to verify tensor dimensions.\\n- Log the attention scores before applying softmax to check for extreme values.\\n- Consider using assertions to ensure that the shapes of the tensors are as expected.\\n\\nBy being mindful of these common mistakes, you can enhance the reliability and performance of your self-attention implementations.\\n\\n## Performance Considerations for Self-Attention\\n\\nSelf-attention mechanisms have revolutionized natural language processing (NLP), but they come with significant computational costs. Understanding these costs is crucial for optimizing performance in real-world applications.\\n\\n### Time and Space Complexity\\n\\nThe time complexity of self-attention is \\\\(O(n^2 \\\\cdot d)\\\\), where \\\\(n\\\\) is the sequence length and \\\\(d\\\\) is the dimensionality of the input embeddings. This quadratic relationship arises because each token in the sequence attends to every other token, resulting in a full attention matrix of size \\\\(n \\\\times n\\\\). The space complexity is also \\\\(O(n^2)\\\\) due to the storage of this attention matrix.\\n\\nFor example, consider a sequence of 512 tokens with an embedding size of 768. The attention matrix would require approximately 196,608,000 entries, which can be prohibitive for longer sequences.\\n\\n### Optimization Strategies\\n\\nTo mitigate the high computational costs, several strategies can be employed:\\n\\n- **Sparse Attention**: Instead of computing attention for all pairs of tokens, sparse attention mechanisms focus on a subset. Techniques like local attention (only attending to nearby tokens) or global attention (attending to specific tokens) can reduce complexity to \\\\(O(n \\\\cdot d \\\\cdot k)\\\\), where \\\\(k\\\\) is the number of attended tokens.\\n\\n- **Kernelized Methods**: These methods approximate the attention mechanism using kernel functions, reducing the complexity to linear or sub-quadratic. For instance, the Performer model uses positive orthogonal random features to approximate softmax attention, achieving \\\\(O(n \\\\cdot d \\\\cdot \\\\log(n))\\\\) complexity.\\n\\n- **Low-Rank Approximations**: Techniques like Linformer use low-rank projections to reduce the size of the attention matrix, allowing for faster computations while maintaining performance.\\n\\n### Benchmarks Against RNNs and LSTMs\\n\\nWhen comparing self-attention with traditional recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), benchmarks reveal significant differences in performance:\\n\\n- **Translation Tasks**: In machine translation, self-attention models like Transformers outperform LSTMs by achieving higher BLEU scores while training in parallel, significantly reducing training time.\\n\\n- **Text Classification**: Self-attention models can process entire sequences simultaneously, leading to faster inference times compared to RNNs, which must process tokens sequentially.\\n\\n- **Memory Usage**: While RNNs and LSTMs have linear memory requirements, self-attention\\'s quadratic memory usage can become a bottleneck for long sequences. \\n\\n### Trade-offs and Edge Cases\\n\\nWhile self-attention provides superior performance in many tasks, it is essential to consider trade-offs. Sparse and kernelized methods may introduce approximation errors, potentially affecting model accuracy. Additionally, for very long sequences, the memory overhead can lead to out-of-memory errors. \\n\\nTo handle these edge cases, consider implementing gradient checkpointing to save memory during training or using mixed precision training to reduce memory footprint. \\n\\nIn summary, understanding the performance implications of self-attention is vital for effective model design and deployment in NLP applications.\\n\\n## Testing and Observability in Self-Attention Models\\n\\nTo ensure the reliability and performance of self-attention models in production, it is crucial to track specific metrics that provide insights into their behavior. Key metrics to monitor include:\\n\\n- **Attention Distribution**: Analyze how attention weights are distributed across different tokens. This helps in understanding which parts of the input the model focuses on.\\n- **Loss Curves**: Track training and validation loss over epochs. A decreasing loss indicates that the model is learning effectively, while a plateau or increase may signal issues such as overfitting.\\n\\nVisualizing attention weights is essential for interpreting model behavior. By plotting attention distributions, you can identify patterns and anomalies. For instance, if a model consistently attends to irrelevant tokens, it may indicate a need for further training or architectural adjustments. Use libraries like Matplotlib or Seaborn to create heatmaps of attention weights:\\n\\n```python\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndef plot_attention_weights(attention_weights):\\n    plt.figure(figsize=(10, 8))\\n    sns.heatmap(attention_weights, cmap=\\'viridis\\')\\n    plt.title(\\'Attention Weights Heatmap\\')\\n    plt.xlabel(\\'Input Tokens\\')\\n    plt.ylabel(\\'Output Tokens\\')\\n    plt.show()\\n```\\n\\nSetting up logging and monitoring for self-attention models in a production environment is critical for maintaining performance. Hereâ€™s a checklist to guide you:\\n\\n1. **Log Model Predictions**: Capture input data, predictions, and actual outcomes to analyze model performance over time.\\n2. **Monitor Latency**: Measure the time taken for inference requests to ensure they meet performance requirements.\\n3. **Track Resource Utilization**: Monitor CPU, GPU, and memory usage to identify potential bottlenecks.\\n4. **Set Up Alerts**: Configure alerts for significant deviations in loss or latency metrics to respond quickly to issues.\\n5. **Version Control**: Maintain versioning of models and datasets to facilitate rollback in case of performance degradation.\\n\\nBy implementing these practices, you can enhance the observability of self-attention models, allowing for timely interventions and continuous improvement. This proactive approach not only boosts reliability but also aids in understanding model behavior, ultimately leading to better performance in real-world applications.\\n\\n## Conclusion and Next Steps\\n\\nSelf-attention has revolutionized natural language processing (NLP) by allowing models to weigh the significance of different words in a sentence relative to each other. Unlike traditional methods, which often rely on fixed context windows or sequential processing, self-attention enables models to capture long-range dependencies and contextual relationships more effectively. This leads to improved performance in tasks such as translation, summarization, and sentiment analysis.\\n\\nFor those looking to deepen their understanding of self-attention, consider exploring advanced topics such as:\\n\\n- **Transformers**: The architecture that popularized self-attention, enabling parallel processing of input sequences.\\n- **BERT (Bidirectional Encoder Representations from Transformers)**: A model that leverages self-attention for context-aware embeddings, significantly enhancing various NLP tasks.\\n\\nTo implement self-attention in your projects, follow this checklist:\\n\\n1. **Understand the Mechanism**: Familiarize yourself with the self-attention formula:\\n   ```python\\n   Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V\\n   ```\\n2. **Choose a Framework**: Select a deep learning framework like TensorFlow or PyTorch that supports self-attention layers.\\n3. **Preprocess Data**: Tokenize and encode your text data appropriately, ensuring that you handle padding and masking.\\n4. **Implement the Layer**: Use built-in self-attention layers or create a custom implementation based on your needs.\\n5. **Tune Hyperparameters**: Experiment with the number of attention heads, layer sizes, and dropout rates to optimize performance.\\n6. **Evaluate and Iterate**: Continuously assess model performance on validation datasets and refine your approach.\\n\\nFor further resources, consider the original paper on Transformers, online courses on NLP, and GitHub repositories with self-attention implementations. Understanding these concepts will enhance your ability to leverage self-attention effectively in your projects.\\n'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ee62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blogGenration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
